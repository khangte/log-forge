services:
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    # restart: unless-stopped
    ports: ["29092:29092"]
    environment:
      # KRaft 필수
      CLUSTER_ID: "nkfq3mlSTrG7l5xDu8Ks9w"
      # KRaft 역할/노드/리스너
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092,CONTROLLER://0.0.0.0:9093"
      # 외부 접속 필요 시 localhost → <VM_IP> 로 교체
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      # 단일 브로커 기본값
      # RF=1 (복제 없음. 장애 시 데이터 유실 위험 큼)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      # min_isr=1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # 파티션 수
      KAFKA_NUM_PARTITIONS: 8
      # ★ 로그/메타데이터 경로(메타데이터 경로를 명시적으로 지정)
      KAFKA_LOG_DIRS: "/var/lib/kafka/logs"
      KAFKA_METADATA_LOG_DIR: "/var/lib/kafka/metadata"
      # ★ JVM 힙(정수 단위로)
      # 초기 크기 1GB, 최대 크기 1.5GB
      KAFKA_HEAP_OPTS: "-Xms1g -Xmx1536m"
      # 토픽별 retention 감수
      # 2시간만 보관 + 세그먼트 작게
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_RETENTION_BYTES: 268435456    # 256MB
      KAFKA_LOG_SEGMENT_BYTES: 134217728      # 128MB
    volumes:
      - /data/log-etlm/kafka-logs:/var/lib/kafka/logs
      - /data/log-etlm/kafka-meta:/var/lib/kafka/metadata
    ulimits:
      nofile: { soft: 100000, hard: 100000 }
    networks: [logetlm-net]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }
    healthcheck:
      # 고부하/디스크 IO 상황에서 kafka-* CLI가 오래 걸리거나 hang 될 수 있어
      # 포트 오픈 여부로 가볍게 체크한다.
      test: ["CMD-SHELL", "timeout 5s bash -lc 'echo > /dev/tcp/localhost/9092' >/dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # kafka-ui:
  #   image: provectuslabs/kafka-ui:latest
  #   container_name: kafka-ui
  #   # restart: unless-stopped
  #   ports: ["8080:8080"]
  #   environment:
  #     # 클러스터 설정: 내부 브로커 주소 사용
  #     - KAFKA_CLUSTERS_0_NAME=local
  #     - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
  #     # (선택) 토픽 생성 허용
  #     - KAFKA_CLUSTERS_0_READONLY=false
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   networks: [logetlm-net]
  #   logging:
  #     driver: json-file
  #     options: { max-size: "10m", max-file: "5" }

  simulator:
    # 시뮬레이터 컨테이너(옵션). log_gateway FastAPI 앱을 uvicorn으로 실행하면서
    # 백그라운드 로그 제너레이터가 Kafka로 이벤트를 전송한다.
    build:
      context: .
      dockerfile: log_gateway/Dockerfile
    container_name: simulator
    # restart: unless-stopped
    ports: ["8000:8000"]
    environment:
      - KAFKA_CLIENT_ID=log-monitoring-simulator
      - KAFKA_BOOTSTRAP=kafka:9092
      - EPS_PER_WORKER_OVERRIDE=625
      - PUBLISHER_WORKERS=4
      - WORKER_BATCH_SIZE=400
      - PRODUCER_EXECUTOR=16
      - QUEUE_SIZE=4000
      - LOG_BATCH_SIZE=200
      - LOOPS_PER_SERVICE=4
      - QUEUE_LOW_SLEEP_SCALE=1.0
      - PRODUCER_LINGER_MS=20
      - PRODUCER_BATCH_NUM_MESSAGES=5000
      - PRODUCER_QUEUE_MAX_KBYTES=262144
      - PRODUCER_QUEUE_MAX_MESSAGES=2000000
      - PRODUCER_ENABLE_IDEMPOTENCE=false
      - PRODUCER_ACKS=0
      - PRODUCER_COMPRESSION=none
      - PRODUCER_POLL_SLEEP_SEC=0.01
      - TICK_SEC=0.3
      # 로그 과다 방지(성능 측정 시 중요)
      - SEND_WARN_SEC=5
      - IDLE_WARN_SEC=5
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./log_gateway:/app/log_gateway:ro
    networks: [logetlm-net]
    command: ["uvicorn", "log_gateway.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  simulator2:
    build:
      context: .
      dockerfile: log_gateway/Dockerfile
    container_name: simulator2
    # restart: unless-stopped
    environment:
      - KAFKA_CLIENT_ID=log-monitoring-simulator-2
      - KAFKA_BOOTSTRAP=kafka:9092
      - EPS_PER_WORKER_OVERRIDE=625
      - PUBLISHER_WORKERS=4
      - WORKER_BATCH_SIZE=400
      - PRODUCER_EXECUTOR=16
      - QUEUE_SIZE=4000
      - LOG_BATCH_SIZE=200
      - LOOPS_PER_SERVICE=4
      - QUEUE_LOW_SLEEP_SCALE=1.0
      - PRODUCER_LINGER_MS=20
      - PRODUCER_BATCH_NUM_MESSAGES=5000
      - PRODUCER_QUEUE_MAX_KBYTES=262144
      - PRODUCER_QUEUE_MAX_MESSAGES=2000000
      - PRODUCER_ENABLE_IDEMPOTENCE=false
      - PRODUCER_ACKS=0
      - PRODUCER_COMPRESSION=none
      - PRODUCER_POLL_SLEEP_SEC=0.01
      - TICK_SEC=0.3
      - SEND_WARN_SEC=5
      - IDLE_WARN_SEC=5
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./log_gateway:/app/log_gateway:ro
    networks: [logetlm-net]
    command: ["uvicorn", "log_gateway.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  # simulator3:
  #   build:
  #     context: .
  #     dockerfile: log_gateway/Dockerfile
  #   container_name: simulator3
  #   # restart: unless-stopped
  #   environment:
  #     - KAFKA_CLIENT_ID=log-monitoring-simulator-3
  #     - KAFKA_BOOTSTRAP=kafka:9092
  #     - EPS_PER_WORKER_OVERRIDE=750
  #     - PUBLISHER_WORKERS=4
  #     - WORKER_BATCH_SIZE=400
  #     - PRODUCER_EXECUTOR=16
  #     - QUEUE_SIZE=4000
  #     - LOG_BATCH_SIZE=200
  #     - LOOPS_PER_SERVICE=4
  #     - QUEUE_LOW_SLEEP_SCALE=1.0
  #     - PRODUCER_LINGER_MS=20
  #     - PRODUCER_BATCH_NUM_MESSAGES=5000
  #     - PRODUCER_QUEUE_MAX_KBYTES=262144
  #     - PRODUCER_QUEUE_MAX_MESSAGES=2000000
  #     - PRODUCER_ENABLE_IDEMPOTENCE=false
  #     - PRODUCER_ACKS=0
  #     - PRODUCER_COMPRESSION=none
  #     - PRODUCER_POLL_SLEEP_SEC=0.01
  #     - TICK_SEC=0.2
  #     - SEND_WARN_SEC=5
  #     - IDLE_WARN_SEC=5
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   volumes:
  #     - ./log_gateway:/app/log_gateway:ro
  #   networks: [logetlm-net]
  #   command: ["uvicorn", "log_gateway.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
  #   logging:
  #     driver: json-file
  #     options: { max-size: "10m", max-file: "5" }


  spark-master:
    image: spark:4.0.1-python3
    container_name: spark-master
    # restart: unless-stopped
    command:
      - "/opt/spark/bin/spark-class"
      - "org.apache.spark.deploy.master.Master"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "7077"
      - "--webui-port"
      - "8081"
    ports:
      - 7077:7077
      - 8081:8081
    networks: [logetlm-net]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  spark-worker-1:
    image: spark:4.0.1-python3
    # restart: unless-stopped
    depends_on:
      - spark-master
    command:
      - "/opt/spark/bin/spark-class"
      - "org.apache.spark.deploy.worker.Worker"
      - "spark://spark-master:7077"
      - "--cores"
      - "3"
      - "--memory"
      - "3g"
    networks: [logetlm-net]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  spark-worker-2:
    image: spark:4.0.1-python3
    # restart: unless-stopped
    depends_on:
      - spark-master
    command:
      - "/opt/spark/bin/spark-class"
      - "org.apache.spark.deploy.worker.Worker"
      - "spark://spark-master:7077"
      - "--cores"
      - "3"
      - "--memory"
      - "3g"
    networks: [logetlm-net]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }


  spark:
    build:
      context: .
      dockerfile: spark_job/Dockerfile
    container_name: spark
    # restart: unless-stopped
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - SPARK_MASTER_URL=spark://spark-master:7077
      # Spark Streaming ingest tuning
      # maxOffsetsPerTrigger가 consumed EPS 상한을 만든다.
      # 예) 배치 주기(dt)가 35s면 250k/35s ≈ 7.1k EPS로 캡이 걸림.
      # 10k EPS 목표면 최소 10k*dt 이상으로 올려야 함(여유 포함).
      # 너무 크게 잡으면 배치가 커져 처리시간(dt)이 늘면서 오히려 EPS가 떨어질 수 있음.
      # 현재 VM 리소스 기준으로 35만부터 시작해서 측정값(consume/lag)에 따라 올리거나 내린다.
      - SPARK_MAX_OFFSETS_PER_TRIGGER=400000
      # Kafka startingOffsets: latest | earliest | (topic별 JSON 문자열)
      - SPARK_STARTING_OFFSETS=latest
      # 체크포인트 손상 시(예: Incomplete log file) 자동 초기화 후 latest부터 재시작 (실시간 우선)
      - SPARK_RESET_CHECKPOINT_ON_START=False
      # ClickHouse sink tuning (override as needed)
      - SPARK_CLICKHOUSE_WRITE_PARTITIONS=3
      - SPARK_CLICKHOUSE_JDBC_BATCHSIZE=60000
      # 배치마다 count/min/max 집계는 처리량을 크게 깎을 수 있어 샘플링(예: 10배치마다 1번)로 제한
      - SPARK_CLICKHOUSE_LOG_STATS=false
      # 원본 JSON(raw_json) 저장 여부
      - SPARK_STORE_RAW_JSON=false
    ports: ["4040:4040"]   # Spark UI
    volumes:
      - ./spark_job:/app/spark_job:ro
      - /data/log-etlm/spark_checkpoints:/data/spark_checkpoints
    depends_on:
      - kafka
      - clickhouse
      - spark-master
      - spark-worker-1
      - spark-worker-2
    networks: [logetlm-net]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:4040/api/v1/applications >/dev/null || exit 1"]
      interval: 45s
      timeout: 5s
      retries: 5
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    # restart: unless-stopped
    environment:
      - CLICKHOUSE_USER=log_user
      - CLICKHOUSE_PASSWORD=log_pwd
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      # NOTE: ClickHouse 데이터는 루트(/)가 아니라 /data에 둬야 디스크(루트) 고갈로 ingest가 멈추는 걸 방지할 수 있음
      - /data/log-etlm/clickhouse:/var/lib/clickhouse
      - /data/log-etlm/clickhouse-logs:/var/log/clickhouse-server
      - ./spark_job/warehouse/create_tables.sql:/docker-entrypoint-initdb.d/create_tables.sql:ro
      - ./spark_job/warehouse/clickhouse_grafana_grants.sql:/docker-entrypoint-initdb.d/clickhouse_grafana_grants.sql:ro
    networks: [logetlm-net]
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --host localhost --query 'SELECT 1' >/dev/null"]
      interval: 45s
      timeout: 5s
      retries: 5
      start_period: 45s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  grafana:
    image: grafana/grafana:10.4.2
    container_name: grafana
    # restart: unless-stopped
    ports: ["3000:3000"]
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clickhouse-datasource
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/etc/grafana/dashboards:ro
      # NOTE: Grafana sqlite/db 및 캐시 파일도 /data로 분리
      - /data/log-etlm/grafana:/var/lib/grafana
    depends_on:
      clickhouse:
        condition: service_started
    networks: [logetlm-net]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

networks:
  logetlm-net:
    driver: bridge
