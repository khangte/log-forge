# -----------------------------------------------------------------------------
# 파일명 : docker-compose.yml
# 목적   : Kafka(단일 KRaft) + Kafka UI + log_gateway FastAPI 시뮬레이터를 통합 기동
# 사용법 :
#   # 1) 브로커 + UI만 띄우기
#   docker compose up -d kafka kafka-ui
#
#   # 2) 시뮬레이터(API + 로그 제너레이터)가 Kafka로 이벤트를 쏘도록 함께 실행
#   docker compose up -d kafka kafka-ui simulator
#   # simulator 컨테이너 내 API 핸들러/백그라운드 제너레이터는 uvicorn + run_generator 로 구동
#
# 포트 :
#   - Kafka 외부접속(호스트): 29092
#   - Kafka 내부접속(컨테이너 간): kafka:9092
#   - Kafka UI: http://localhost:8080
# -----------------------------------------------------------------------------
services:
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    restart: unless-stopped
    ports: ["29092:29092"]
    environment:
      # KRaft 필수
      CLUSTER_ID: "nkfq3mlSTrG7l5xDu8Ks9w"
      # KRaft 역할/노드/리스너
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092,CONTROLLER://0.0.0.0:9093"
      # 외부 접속 필요 시 localhost → <VM_IP> 로 교체
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      # 단일 브로커 기본값
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # ★ 로그/메타데이터 경로(메타데이터 경로를 명시적으로 지정)
      KAFKA_LOG_DIRS: "/var/lib/kafka/logs"
      KAFKA_METADATA_LOG_DIR: "/var/lib/kafka/metadata"
      # ★ JVM 힙(정수 단위로)
      # 초기 크기 1GB, 최대 크기 1.5GB
      KAFKA_HEAP_OPTS: "-Xms1g -Xmx1536m"
    volumes:
      - /data/kafka-logs:/var/lib/kafka/logs
      - /data/kafka-meta:/var/lib/kafka/metadata
    ulimits:
      nofile: { soft: 100000, hard: 100000 }
    networks: [logmonitoring-net]
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports: ["8080:8080"]
    environment:
      # 클러스터 설정: 내부 브로커 주소 사용
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      # (선택) 토픽 생성 허용
      - KAFKA_CLUSTERS_0_READONLY=false
    depends_on:
      kafka:
        condition: service_healthy
    networks: [logmonitoring-net]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  simulator:
    # 시뮬레이터 컨테이너(옵션). log_gateway FastAPI 앱을 uvicorn으로 실행하면서
    # 백그라운드 로그 제너레이터가 Kafka로 이벤트를 전송한다.
    build:
      context: .
      dockerfile: log_gateway/Dockerfile
    container_name: simulator
    restart: unless-stopped
    ports: ["8000:8000"]
    environment:
      - KAFKA_CLIENT_ID=log-monitoring-simulator
      - KAFKA_BOOTSTRAP=kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    # 개발 시 로컬 수정 반영이 필요하면 아래 볼륨을 해제해 사용(이미지의 /app/log_gateway 덮어쓰기)
    volumes:
      - ./log_gateway:/app/log_gateway:ro
    networks: [logmonitoring-net]
    command: ["uvicorn", "log_gateway.main:app", "--host", "0.0.0.0", "--port", "8000"]
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "5" }

  spark:
    build:
      context: .
      dockerfile: spark_job/Dockerfile
    container_name: spark
    restart: unless-stopped
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
    ports: ["4040:4040"]   # Spark UI
    volumes:
      - ./spark_job:/app/spark_job:ro
      - /data/spark_checkpoints:/data/spark_checkpoints
    depends_on:
      - kafka
      - clickhouse
    networks: [logmonitoring-net]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:4040/api/v1/applications >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    restart: unless-stopped
    environment:
      - CLICKHOUSE_USER=log_user
      - CLICKHOUSE_PASSWORD=log_pwd
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./data/clickhouse/data:/var/lib/clickhouse
      - ./data/clickhouse/logs:/var/log/clickhouse-server
      - ./spark_job/warehouse/create_tables.sql:/docker-entrypoint-initdb.d/create_tables.sql:ro
    networks: [logmonitoring-net]
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --host localhost --query 'SELECT 1' >/dev/null"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 45s

  grafana:
    image: grafana/grafana:10.4.2
    container_name: grafana
    restart: unless-stopped
    ports: ["3000:3000"]
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clickhouse-datasource
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/etc/grafana/dashboards:ro
      - ./data/grafana:/var/lib/grafana
    depends_on:
      clickhouse:
        condition: service_started
    networks: [logmonitoring-net]

networks:
  logmonitoring-net:
    driver: bridge
