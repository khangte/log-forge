# -----------------------------------------------------------------------------
# íŒŒì¼ëª… : log_gateway/pipeline.py
# ëª©ì    : ì„œë¹„ìŠ¤ë³„ ë°°ì¹˜ ìƒì„± ë£¨í”„ ë° Kafka í¼ë¸”ë¦¬ì…” íƒœìŠ¤í¬ êµ¬ì„±
# ì„¤ëª…   : generatorê°€ ì „ë‹¬í•œ í”„ë¡œíŒŒì¼/ì‹œë®¬ë ˆì´í„° ê¸°ë°˜ìœ¼ë¡œ í+ì›Œì»¤ íë¦„ì„ ì‹¤í–‰
# -----------------------------------------------------------------------------
from __future__ import annotations

import asyncio
import time
from typing import Any, Dict, List, Tuple

from .producer import get_producer, publish_batch
from .config.timeband import current_hour_kst, pick_multiplier

# ===== íŒŒì´í”„ë¼ì¸(ìƒì„±/ì „ì†¡) íŒŒë¼ë¯¸í„° =====
LOG_BATCH_SIZE : int = 200
QUEUE_SIZE : int = 10000
PUBLISHER_WORKERS : int = 8
WORKER_BATCH_SIZE : int = 800
POLL_EVERY = 50

# # í¼ë¸”ë¦¬ì…” íŠœë‹(ë¯¸ë‹ˆë°°ì¹˜ ë“œë ˆì¸/í´ë§/ë°±ì˜¤í”„)
# WORKER_DRAIN_COUNT: int = int(os.getenv("LG_WORKER_DRAIN_COUNT", "5000"))
# WORKER_DRAIN_MS: int = int(os.getenv("LG_WORKER_DRAIN_MS", "5"))
# BUFFER_BACKOFF_MS: int = int(os.getenv("LG_BUFFER_BACKOFF_MS", "5"))


async def _service_stream_loop(
    service: str,
    simulator: Any,
    target_rps: float,
    publish_queue: "asyncio.Queue[Tuple[str, str, bool]]",
    bands: List[Any],
    weight_mode: str,
    # batch_range: Tuple[int, int],
    log_batch_size: int
) -> None:
    """ì„œë¹„ìŠ¤ë³„ë¡œ ë°°ì¹˜ ë¡œê·¸ë¥¼ ìƒì„±í•´ í¼ë¸”ë¦¬ì‹œ íì— ìŒ“ëŠ”ë‹¤."""
    while True:
        loop_start = time.perf_counter()
        hour = current_hour_kst()  # í˜„ì¬ ì‹œê°„ëŒ€(KST) ê²°ì •
        multiplier = pick_multiplier(bands, hour_kst=hour, mode=weight_mode) if bands else 1.0  # ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜ ì ìš©
        effective_rps = max(target_rps * multiplier, 0.01)  # ëª©í‘œ RPS Ã— multiplier
        batch_size = log_batch_size

        logs = simulator.generate_logs(batch_size)  # ì‹œë®¬ë ˆì´í„°ì—ì„œ ë¡œê·¸ ë°°ì¹˜ ìƒì„±
        for log in logs:
            payload = simulator.render(log)
            await publish_queue.put((service, payload, log.get("level") == "ERROR"))

        desired_period = batch_size / effective_rps
        elapsed = time.perf_counter() - loop_start
        sleep_time = max(0.0, desired_period - elapsed)

        if sleep_time > 0:
            await asyncio.sleep(sleep_time)


async def _publisher_worker(
    worker_id: int,
    publish_queue: "asyncio.Queue[Tuple[str, str, bool]]",
    stats_queue: "asyncio.Queue[Tuple[str, int]]",
) -> None:
    """íì— ìŒ“ì¸ ë¡œê·¸ë¥¼ Kafkaì— ë°œí–‰"""
    producer = get_producer()

    while True:
        batch = []

        # ìµœì†Œ 1ê±´
        batch.append(await publish_queue.get())

        # WORKER_BATCH_SIZE-1ê°œ ì¶”ê°€ drain
        for _ in range(WORKER_BATCH_SIZE - 1):
            try:
                batch.append(publish_queue.get_nowait())
            except asyncio.QueueEmpty:
                break

        # ë°°ì¹˜ë¥¼ í•œ ë²ˆì— thread pool ë¡œ ë„˜ê²¨ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹˜ ê°ì†Œ
        await publish_batch(
            [(service, payload, None, err) for (service, payload, err) in batch]
        )

        # batch ë‚´ 50ê°œë§ˆë‹¤ poll, ê·¸ í›„ ë§ˆì§€ë§‰ì— poll 1íšŒ
        processed = 0
        for _ in batch:
            processed += 1
            if processed % POLL_EVERY == 0:
                producer.poll(0)

        # ë§ˆì§€ë§‰ì—ë„ í•œ ë²ˆ ë” poll â†’ ì¹´í”„ì¹´ ë‚´ë¶€ í ëˆ„ì  ë°©ì§€
        producer.poll(0)

        # --- ğŸ”¥ ì„œë¹„ìŠ¤ë³„ ì¹´ìš´íŠ¸ ì§‘ê³„ ---
        svc_counter = {}
        for (svc, _, _) in batch:
            svc_counter[svc] = svc_counter.get(svc, 0) + 1

        # --- ğŸ”¥ stats_queueì— ì„œë¹„ìŠ¤ë³„ë¡œ push ---
        for svc, cnt in svc_counter.items():
            stats_queue.put_nowait((svc, cnt))

        # --- task_done ì²˜ë¦¬ ---
        for _ in batch:
            publish_queue.task_done()


def start_pipeline(
    simulators: Dict[str, Any],
    base_rps: float,
    bands: List[Any],
    service_rps: Dict[str, float],
    weight_mode: str,
) -> Tuple[
    "asyncio.Queue[Tuple[str, str, bool]]",
    "asyncio.Queue[Tuple[str, int]]",
    List[asyncio.Task],
    List[asyncio.Task],
]:
    """í/ì›Œì»¤ íƒœìŠ¤í¬ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜."""
    log_batch_size = LOG_BATCH_SIZE
    publish_queue: "asyncio.Queue[Tuple[str, str, bool]]" = asyncio.Queue(maxsize=QUEUE_SIZE)  # Kafka ì „ì†¡ ëŒ€ê¸° í
    stats_queue: "asyncio.Queue[Tuple[str, int]]" = asyncio.Queue()  # RPS ê³„ì‚°ìš© í

    available_services = list(simulators.keys())
    service_count = max(len(available_services), 1)
    fallback_rps = base_rps / service_count  # mixì— ì—†ëŠ” ì„œë¹„ìŠ¤ ëŒ€ë¹„ ê¸°ë³¸ RPS

    service_tasks = [
        asyncio.create_task(
            _service_stream_loop(
                service=service,
                simulator=simulators[service],
                target_rps=service_rps.get(service, fallback_rps),
                publish_queue=publish_queue,
                bands=bands,
                weight_mode=weight_mode,
                log_batch_size=log_batch_size,
            ),
            name=f"service-loop-{service}",
        )
        for service in available_services
    ]

    publisher_tasks = [
        asyncio.create_task(
            _publisher_worker(worker_id=i, publish_queue=publish_queue, stats_queue=stats_queue),
            name=f"publisher-{i}",
        )
        for i in range(PUBLISHER_WORKERS)
    ]

    return publish_queue, stats_queue, service_tasks, publisher_tasks
