import os
import traceback


def write_to_clickhouse(df, table_name, batch_id: int | None = None):
    cached = False

    try:
        target_partitions = os.getenv("SPARK_CLICKHOUSE_WRITE_PARTITIONS")
        jdbc_batchsize = os.getenv("SPARK_CLICKHOUSE_JDBC_BATCHSIZE", "50000")
        clickhouse_url = os.getenv(
            "SPARK_CLICKHOUSE_URL",
            "jdbc:clickhouse://clickhouse:8123/analytics?compress=0&decompress=0&jdbcCompliant=false&async_insert=1&wait_for_async_insert=0",
            # async_insert=1 + wait_for_async_insert=0:
            # - Spark micro-batchì—ì„œ INSERT ì‘ë‹µ ëŒ€ê¸°ë¥¼ ì¤„ì—¬ ì²˜ë¦¬ëŸ‰ì„ ì˜¬ë¦°ë‹¤.
            # - ëŒ€ì‹œë³´ë“œ/ë¶„ì„ì€ MV ì§‘ê³„ í…Œì´ë¸”ë¡œ ë³´ë¯€ë¡œ, ì•½ê°„ì˜ ì§€ì—°ì€ í—ˆìš© ê°€ëŠ¥.
        )
        clickhouse_user = os.getenv("SPARK_CLICKHOUSE_USER", "log_user")
        clickhouse_password = os.getenv("SPARK_CLICKHOUSE_PASSWORD", "log_pwd")

        out_df = df
        if target_partitions and target_partitions.strip():
            n = int(target_partitions)
            current = out_df.rdd.getNumPartitions()
            if n < current:
                # ì…”í”Œ ì—†ì´ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì—¬ ì“°ê¸° ì˜¤ë²„í—¤ë“œë¥¼ ë‚®ì¶˜ë‹¤.
                out_df = out_df.coalesce(n)
            elif n > current:
                # ë³‘ë ¬ ì“°ê¸°ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ íŒŒí‹°ì…˜ì„ ì¬ë¶„ë°°í•œë‹¤.
                out_df = out_df.repartition(n)

        writer = (
            out_df.write
            .format("jdbc") \
            .option("driver", "com.clickhouse.jdbc.ClickHouseDriver") \
            .option("url", clickhouse_url) \
            .option("user", clickhouse_user) \
            .option("password", clickhouse_password) \
            .option("dbtable", table_name) \
            .option("isolationLevel", "NONE") \
            .option("batchsize", jdbc_batchsize)
            .mode("append")
        )
        writer.save()

    except Exception as e:
        print(f"[âŒ ERROR] ClickHouse ì €ì¥ ì‹¤íŒ¨: {table_name} {e}")
        msg = str(e)
        if "TABLE_ALREADY_EXISTS" in msg and "detached" in msg.lower():
            print(
                "[ğŸ› ï¸ ClickHouse] í…Œì´ë¸”ì´ DETACHED ìƒíƒœì…ë‹ˆë‹¤. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ ë³µêµ¬í•˜ì„¸ìš”:\n"
                "  sudo docker exec -it clickhouse clickhouse-client -u log_user --password log_pwd \\\n"
                "    --query \"ATTACH TABLE analytics.fact_log\""
            )
        traceback.print_exc()
    finally:
        try:
            if cached:
                out_df.unpersist()
        except Exception:
            pass
